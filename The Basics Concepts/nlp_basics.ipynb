{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy x Nltk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Functionality: NLTK is a general-purpose NLP library that provides a wide range of tools and algorithms for text processing, including tokenization, POS tagging, stemming, and sentiment analysis, among others. spaCy, on the other hand, is a more specialized NLP library that focuses on advanced text processing tasks, such as named entity recognition, dependency parsing, and text classification.\n",
    "\n",
    "* Performance: spaCy is known for its speed and efficiency, thanks to its use of Cython, a programming language that is optimized for high-performance computing. NLTK, on the other hand, may be slower in some cases, especially when dealing with large datasets or complex text processing tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is the process of breaking down text into individual words or phrases, known as tokens. Tokenization is a crucial step in natural language processing (NLP) because it is the first step in preparing text for analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'sample',\n",
       " 'sentence',\n",
       " '.',\n",
       " 'And',\n",
       " 'another',\n",
       " 'sentence',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This is a sample sentence. And another sentence.\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t2\n",
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "['another', 'here', 'is', 'sample', 'sentence', 'this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "tokens = vectorizer.fit_transform([text])\n",
    "\n",
    "print(tokens)\n",
    "print(vectorizer.get_feature_names())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', '.', 'Another', 'sentence', 'here', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "tokens = [token.text for token in doc]\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "Stemming is the process of reducing a word to its root or base form, known as the stem. This is achieved by removing suffixes and prefixes from the word. Stemming is a common preprocessing step in natural language processing (NLP) that helps reduce the dimensionality of text data and improve the accuracy of text analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porter stemming is one of the most widely used stemming algorithms in NLP. It is based on a set of heuristic rules that are applied recursively to a word until a suffix is removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play -> play\n",
      "playing -> play\n",
      "played -> play\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words = [\"play\", \"playing\", \"played\"]\n",
    "for word in words:\n",
    "    stem = stemmer.stem(word)\n",
    "    print(f\"{word} -> {stem}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Snowball stemmer (also known as the Porter2 stemmer) is an improved version of the Porter stemmer that is more aggressive in removing suffixes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jumping -> jump\n",
      "jumps -> jump\n",
      "jumped -> jump\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "words = [\"jumping\", \"jumps\", \"jumped\"]\n",
    "for word in words:\n",
    "    stem = stemmer.stem(word)\n",
    "    print(f\"{word} -> {stem}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lancaster stemmer is the most agressive stemming algorithm that can produce very short stems, that can sometimes lose meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jumping -> jump\n",
      "jumps -> jump\n",
      "jumped -> jump\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "words = [\"jumping\", \"jumps\", \"jumped\"]\n",
    "for word in words:\n",
    "    stem = stemmer.stem(word)\n",
    "    print(f\"{word} -> {stem}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "Lemmatization is a process of reducing words to their base form, known as the lemma, based on their morphological features and their part of speech (POS) in the sentence. The main difference between stemming and lemmatization is that stemming reduces words to their root form by simply removing the suffix, whereas lemmatization produces valid words that are present in the dictionary.\n",
    "\n",
    "Compared to stemming, lemmatization produces more accurate and meaningful results. For example, consider the word \"better\". Stemming would reduce it to \"bett\", which is not a valid word and loses the meaning of the original word. On the other hand, lemmatization would reduce it to \"good\", which is a valid word and preserves the meaning of the original word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "playing -> play\n",
      "plays -> play\n",
      "played -> play\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = [\"playing\", \"plays\", \"played\"]\n",
    "for word in words:\n",
    "    lemma = lemmatizer.lemmatize(word, pos=\"v\")\n",
    "    print(f\"{word} -> {lemma}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tagging, or Part-of-Speech tagging, is the process of assigning each word in a text a particular part-of-speech tag based on its definition and context. The parts of speech include nouns, verbs, adjectives, adverbs, pronouns, prepositions, conjunctions, and interjections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('John', 'PROPN'), ('likes', 'VERB'), ('to', 'PART'), ('watch', 'VERB'), ('movies', 'NOUN'), ('.', 'PUNCT'), ('He', 'PRON'), ('prefers', 'VERB'), ('action', 'NOUN'), ('movies', 'NOUN'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # Load the pre-trained model\n",
    "\n",
    "sentence = \"John likes to watch movies. He prefers action movies.\"\n",
    "\n",
    "# Process the sentence and obtain the POS tags for each token\n",
    "doc = nlp(sentence)\n",
    "pos_tags = [(token.text, token.pos_) for token in doc]\n",
    "\n",
    "# Print the POS tags\n",
    "print(pos_tags)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER Named Entity Recognition\n",
    "\n",
    "Named entity recognition (NER) is a common task in NLP that involves identifying and classifying named entities (e.g., people, organizations, locations) in text\n",
    "\n",
    "Summary\n",
    "\n",
    "* PERSON: People, including fictional.\n",
    "* NORP: Nationalities or religious or political groups.\n",
    "* FAC: Buildings, airports, highways, bridges, etc.\n",
    "* ORG: Companies, agencies, institutions, etc.\n",
    "* GPE: Countries, cities, states.\n",
    "* LOC: Non-GPE locations, mountain ranges, bodies of water.\n",
    "* PRODUCT: Objects, vehicles, foods, etc. (Not services.)\n",
    "* EVENT: Named hurricanes, battles, wars, sports events, etc.\n",
    "* WORK_OF_ART: Titles of books, songs, etc.\n",
    "* LAW: Named documents made into laws.\n",
    "* LANGUAGE: Any named language.\n",
    "* DATE: Absolute or relative dates or periods.\n",
    "* TIME: Times smaller than a day.\n",
    "* PERCENT: Percentage, including \"%\".\n",
    "* MONEY: Monetary values, including unit.\n",
    "* QUANTITY: Measurements, as of weight or distance.\n",
    "* ORDINAL: \"first\", \"second\", etc.\n",
    "* CARDINAL: Numerals that do not fall under another type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('New York City', 'GPE'), ('Google', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # Load the pre-trained model\n",
    "\n",
    "text = \"I live in New York City and work at Google.\"\n",
    "\n",
    "# Process the text and obtain the named entities\n",
    "doc = nlp(text)\n",
    "entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
    "\n",
    "# Print the named entities\n",
    "print(entities)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "Sentiment analysis is the task of analyzing a piece of text to determine whether the author's attitude towards a particular topic or subject is positive, negative, or neutral.\n",
    "\n",
    "Sentiment analysis can be useful for a variety of applications, such as social media monitoring, customer feedback analysis, brand reputation management, and market research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0\n",
      "Negative sentiment!\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = \"I like this product! It's horrible.\"\n",
    "\n",
    "# Create a TextBlob object from the text\n",
    "blob = TextBlob(text)\n",
    "\n",
    "# Obtain the sentiment polarity (a value between -1 and 1)\n",
    "sentiment = blob.sentiment.polarity\n",
    "\n",
    "# Print the sentiment polarity\n",
    "print(sentiment)\n",
    "threshold = 0 \n",
    "\n",
    "if sentiment > threshold:\n",
    "    print(\"Positive sentiment!\")\n",
    "else:\n",
    "    print(\"Negative sentiment!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pycaret[full]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
